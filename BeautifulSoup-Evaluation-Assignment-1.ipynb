{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411213ca-a3e8-4c1d-b524-41fb47cb9969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\mahes\\appdata\\roaming\\python\\python311\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418c74f8-ce37-4196-9380-4f1d620b565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a3b93-deef-4f61-810f-dca3e5ccf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "# https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300c3ae4-d457-4285-88af-78a43b20be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Name Rating Year of Release\n",
      "0                     1. Ship of Theseus    8.0            2012\n",
      "1                              2. Iruvar    8.4            1997\n",
      "2                     3. Kaagaz Ke Phool    7.8            1959\n",
      "3   4. Lagaan: Once Upon a Time in India    8.1            2001\n",
      "4                     5. Pather Panchali    8.2            1955\n",
      "5                           6. Charulata    8.1            1964\n",
      "6                     7. Rang De Basanti    8.1            2006\n",
      "7                               8. Dev.D    7.9            2009\n",
      "8                            9. 3 Idiots    8.4            2009\n",
      "9                             10. Awaara    7.8            1951\n",
      "10                           11. Nayakan    8.7            1987\n",
      "11                         12. Aparajito    8.2            1956\n",
      "12                   13. Pushpaka Vimana    8.6            1987\n",
      "13                            14. Pyaasa    8.3            1957\n",
      "14                     15. Ghatashraddha    7.5            1977\n",
      "15                            16. Sholay    8.1            1975\n",
      "16                          17. Aradhana    7.6            1969\n",
      "17             18. Do Ankhen Barah Haath    8.4            1957\n",
      "18                            19. Bombay    8.1            1995\n",
      "19                      20. Neecha Nagar    6.6            1946\n",
      "20                    21. Do Bigha Zamin    8.3            1953\n",
      "21                         22. Garm Hava    8.0            1974\n",
      "22                            23. Piravi    7.8            1989\n",
      "23                     24. Mughal-E-Azam    8.1            1960\n",
      "24                       25. Amma Ariyan    7.4            1986\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    movie_names = []\n",
    "    ratings = []\n",
    "    release_years = []\n",
    "\n",
    "    movies = soup.find_all('li', class_='ipc-metadata-list-summary-item')\n",
    "\n",
    "    for movie in movies:\n",
    "        title_tag = movie.find('h3', class_='ipc-title__text')\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "        rating_tag = movie.find('span', class_='ipc-rating-star--rating')\n",
    "        rating = rating_tag.get_text(strip=True)\n",
    "\n",
    "        year_tag = movie.find('span', class_='dli-title-metadata-item')\n",
    "        year = year_tag.get_text(strip=True).strip('()') if year_tag else 'N/A'\n",
    "\n",
    "        movie_names.append(title)\n",
    "        ratings.append(rating)\n",
    "        release_years.append(year)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': movie_names,\n",
    "        'Rating': ratings,\n",
    "        'Year of Release': release_years\n",
    "    })\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d02d00-a8bb-4991-bb0b-82c4aafcec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms.\n",
    "# Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e50c4ae-1912-4fb4-ad1d-d84a553cfaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Heading, Dates, Content, Like]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "time.sleep(5)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "    headings = []\n",
    "    dates = []\n",
    "    contents = []\n",
    "    likes = []\n",
    "   \n",
    "    posts = soup.find_all('div', class_='sc-xg7a7d-0')\n",
    "    \n",
    "    for post in posts:\n",
    "\n",
    "        heading_tag = post.find('span', class_='sc-1cvoi1y-0')\n",
    "        heading = heading_tag.get_text(strip=True)\n",
    "\n",
    "        date_tag = post.find('span', class_='sc-iqseJM')\n",
    "        date = date_tag.get_text(strip=True)\n",
    "\n",
    "        content_tag = post.find('div', class_='sc-cfnzm4-0').find('p')\n",
    "        content = content_tag.get_text(strip=True).strip('()') if content_tag else 'N/A'\n",
    "\n",
    "        like_tag = post.find('span', class_='sc-iqseJM')\n",
    "        like = like_tag.get_text(strip=True)\n",
    "        \n",
    "        headings.append(heading)\n",
    "        dates.append(date)\n",
    "        contents.append(content)\n",
    "        likes.append(like)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Heading': headings,\n",
    "        'Dates': dates,\n",
    "        'Content': contents,\n",
    "        'Like': likes\n",
    "    })\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601ebd5-180d-427d-9fca-583ca1c3e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8591ad64-1596-4530-acb6-10024f47b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  \\\n",
      "0                1 BHK House for Rent  In Rajajinagar   \n",
      "1   2 BHK Apartment In Admiralty Square Gopalan, I...   \n",
      "2   2 BHK Flat In Standalone Building  for Rent  I...   \n",
      "3           1 RK Flat In Sb for Rent  In Rajaji Nagar   \n",
      "4   2 BHK Apartment In Kumar Ashraya for Rent  In ...   \n",
      "5   2 BHK House for Rent  In Laksmipuram, Indiranagar   \n",
      "6                      2 BHK for Rent  In Rajajinagar   \n",
      "7                3 BHK House for Rent  In Rajajinagar   \n",
      "8                 1 BHK House for Rent  In Siddhapura   \n",
      "9           2 BHK Flat for Rent  In Sk English School   \n",
      "10  2 BHK Apartment In Alpine Regency, Jayanagar f...   \n",
      "11  1 BHK Apartment In Brigade Gateway for Rent  I...   \n",
      "12     3 BHK House for Rent  In Rajajinagar 5th Block   \n",
      "13  1 BHK Flat In Anjanadri Apartments  for Rent  ...   \n",
      "14  1 RK Apartment In Kerala Pavilion for Rent  In...   \n",
      "15                 1 RK Flat for Rent  In Indiranagar   \n",
      "16           2 BHK House for Rent  In Indiranagar Kfc   \n",
      "17               3 BHK House for Rent  In Rajajinagar   \n",
      "18                 1 RK Flat for Rent  In Rajajinagar   \n",
      "19  2 BHK Apartment In Saraswathi Nivas for Rent  ...   \n",
      "20       2 BHK House for Rent  In Jayanagar 3rd Block   \n",
      "21               1 RK House for Rent  In Indira Nagar   \n",
      "22               1 BHK Flat for Rent  In Rajaji Nagar   \n",
      "23  3 BHK Apartment In Varuna Residency for Rent  ...   \n",
      "24  1 BHK House for Rent  In Indira Nagar 1st Stag...   \n",
      "25               1 BHK House for Rent  In Rajajinagar   \n",
      "\n",
      "                                             Location        Area  \\\n",
      "0   Independent House,  Gayathrinagar near  SVN En...    600 sqft   \n",
      "1   Admiralty Square Gopalan, IndiranagarÂ79, 13th...  1,230 sqft   \n",
      "2   9th Cross Rd 8th cross, HAL 2nd Stage, near  C...    600 sqft   \n",
      "3   Vein Clinic, 60th Cross Road, 5 Block, Rajaji ...    130 sqft   \n",
      "4   Kumar AshrayaÂ194, 9th Cross Rd, 2nd Block, Ja...  1,000 sqft   \n",
      "5   Independent House, Near Digiscan  Shakthi Gana...  1,300 sqft   \n",
      "6   Standalone Building, Near Athmeya polyclinic &...  1,000 sqft   \n",
      "7   Independent House, 2nd Block near  S R M Medicals  1,110 sqft   \n",
      "8   Independent House, KHB Colony near Vijaya College    430 sqft   \n",
      "9   Standalone building, Someshwara Nagar ,Ashoka ...    620 sqft   \n",
      "10  Alpine Regency, JayanagarÂAlpine Regency, 10th...  1,050 sqft   \n",
      "11  Brigade GatewayÂDr Rajkumar Road, Rajaji Nagar...    590 sqft   \n",
      "12  Independent House, 11th Main Rd, 5 Block, Raja...  2,000 sqft   \n",
      "13  2406, 10th Main Rd, E block, 2nd Stage, Rajaji...    500 sqft   \n",
      "14                        Kerala PavilionÂIndiranagar    150 sqft   \n",
      "15  Standalone Building, C.M.H Road, near Metro Pi...    300 sqft   \n",
      "16  Independent House, Defence Colony, sri krishna...  1,000 sqft   \n",
      "17  Independent House, Near Asal Food Products Pvt...  1,400 sqft   \n",
      "18  Standalone building, Near Jyothi Kannada and T...    600 sqft   \n",
      "19           Saraswathi NivasÂMain Channel Rd, Ulsoor  1,100 sqft   \n",
      "20       Independent House, near ramakrishna hospital  1,000 sqft   \n",
      "21  Independent House, Parallel to 100 ft road, ne...    500 sqft   \n",
      "22  Standalone Building, 4th main, Manjunathnagar,...    600 sqft   \n",
      "23  Varuna ResidencyÂChannakesahava Nagar, HAL 2nd...  1,625 sqft   \n",
      "24  Independent House, Indira Nagar 1st Stage, Sta...    450 sqft   \n",
      "25  Independent House,  Gayathrinagar near  SVN En...    600 sqft   \n",
      "\n",
      "                               Emi        Price  \n",
      "0   â¹ 14,000No Extra Maintenance  â¹1,00,000  \n",
      "1   â¹ 60,000+â¹5,000Maintenance  â¹5,40,000  \n",
      "2   â¹ 25,000No Extra Maintenance  â¹1,00,000  \n",
      "3   â¹ 11,000No Extra Maintenance    â¹30,000  \n",
      "4   â¹ 28,000+â¹2,000Maintenance  â¹2,00,000  \n",
      "5   â¹ 30,000+â¹1,000Maintenance  â¹1,25,000  \n",
      "6   â¹ 14,000No Extra Maintenance  â¹1,50,000  \n",
      "7   â¹ 20,000No Extra Maintenance  â¹1,50,000  \n",
      "8   â¹ 10,000No Extra Maintenance    â¹80,000  \n",
      "9   â¹ 15,000No Extra Maintenance  â¹2,00,000  \n",
      "10  â¹ 60,000+â¹4,500Maintenance  â¹3,50,000  \n",
      "11  â¹ 47,000+â¹3,000Maintenance  â¹4,70,000  \n",
      "12  â¹ 34,000No Extra Maintenance  â¹3,40,000  \n",
      "13  â¹ 16,000+â¹1,200Maintenance  â¹1,60,000  \n",
      "14  â¹ 15,000No Extra Maintenance    â¹30,000  \n",
      "15   â¹ 6,000No Extra Maintenance    â¹20,000  \n",
      "16  â¹ 30,000No Extra Maintenance  â¹1,50,000  \n",
      "17  â¹ 25,000No Extra Maintenance  â¹3,00,000  \n",
      "18   â¹ 5,000No Extra Maintenance    â¹25,000  \n",
      "19  â¹ 57,000+â¹2,000Maintenance  â¹2,50,000  \n",
      "20  â¹ 55,000No Extra Maintenance  â¹3,00,000  \n",
      "21  â¹ 20,000+â¹1,000Maintenance    â¹80,000  \n",
      "22  â¹ 10,000No Extra Maintenance  â¹1,00,000  \n",
      "23  â¹ 80,000No Extra Maintenance  â¹2,50,000  \n",
      "24  â¹ 26,500No Extra Maintenance  â¹2,00,000  \n",
      "25  â¹ 14,000No Extra Maintenance  â¹1,00,000  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://www.nobroker.in/property/rent/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9LHsibGF0IjoxMi45MzA3NzM1LCJsb24iOjc3LjU4MzgzMDIsInBsYWNlSWQiOiJDaElKMmRkbFo1Z1ZyanNSaDFCT0FhZi1vcnMiLCJwbGFjZU5hbWUiOiJKYXlhbmFnYXIifSx7ImxhdCI6MTIuOTk4MTczMiwibG9uIjo3Ny41NTMwNDQ1OTk5OTk5OSwicGxhY2VJZCI6IkNoSUp4Zlc0RFBNOXJqc1JLc05URy01cF9RUSIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIn1d&radius=2.0&sharedAccomodation=0&city=bangalore&locality=Indiranagar,Jayanagar,Rajajinagar'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "\n",
    "    articles = soup.find('div', class_='infinite-scroll-component').find_all('article')\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        title_tag = article.find('h2', class_='heading-6')\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "        location_tag = article.find('div', class_='leading-4')\n",
    "        location = location_tag.get_text(strip=True)\n",
    "\n",
    "        area_tag = article.find('div', id='unitCode')\n",
    "        area = area_tag.get_text(strip=True)\n",
    "\n",
    "        emi_tag = article.find('div', id='minimumRent')\n",
    "        emi = emi_tag.get_text(strip=True)\n",
    "\n",
    "        price_tag = article.find('div', id='roomType')\n",
    "        price = price_tag.get_text(strip=True)\n",
    "\n",
    "        titles.append(title)\n",
    "        locations.append(location)\n",
    "        areas.append(area)\n",
    "        emis.append(emi)\n",
    "        prices.append(price)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Location': locations,\n",
    "        'Area': areas,\n",
    "        'Emi': emis,\n",
    "        'Price': prices\n",
    "    })\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a1c0f-2282-4d95-9443-4e068910efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/bestseller?sort=popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94590dff-bab6-445a-8067-2f05805a10c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Title Price  \\\n",
      "0                   bewakoof x dc  ₹499   \n",
      "1  bewakoof x house of the dragon  ₹399   \n",
      "2          bewakoof x tom & jerry  ₹499   \n",
      "3                       Bewakoof®  ₹439   \n",
      "4                       Bewakoof®  ₹499   \n",
      "5              bewakoof x peanuts  ₹599   \n",
      "6                       Bewakoof®  ₹549   \n",
      "7                       Bewakoof®  ₹499   \n",
      "8          bewakoof x tom & jerry  ₹499   \n",
      "9                       Bewakoof®  ₹499   \n",
      "\n",
      "                                           Image Url  \n",
      "0  https://images.bewakoof.com/t640/men-s-black-a...  \n",
      "1  https://images.bewakoof.com/t640/men-s-black-h...  \n",
      "2  https://images.bewakoof.com/t640/women-s-blue-...  \n",
      "3  https://images.bewakoof.com/t640/women-aop-ove...  \n",
      "4  https://images.bewakoof.com/t640/men-s-black-w...  \n",
      "5                   /images/fallback-placeholder.svg  \n",
      "6                   /images/fallback-placeholder.svg  \n",
      "7                   /images/fallback-placeholder.svg  \n",
      "8                   /images/fallback-placeholder.svg  \n",
      "9                   /images/fallback-placeholder.svg  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://shop.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    titles = []\n",
    "    prices = []\n",
    "    urls = []\n",
    "\n",
    "    products = soup.find_all('section', class_='sc-8020ee44-4')\n",
    "    product_limit = 10\n",
    "    count = 0\n",
    "\n",
    "    for product in products:\n",
    "        if count >= product_limit:\n",
    "            break\n",
    "\n",
    "        title_tag = product.find('div', class_='product-card_product_container__NkgVJ').find('span', class_='sc-a6c4ca6a-0')\n",
    "        title = title_tag.get_text(strip=True)    \n",
    "\n",
    "        price_tag = product.find('div', class_='product-card_product_price_container__Ek01t').find('span', class_='sc-a6c4ca6a-0')\n",
    "        price = price_tag.get_text(strip=True)\n",
    "\n",
    "        url_tag = product.find('img')\n",
    "        url = url_tag.get('src') if url_tag else None \n",
    "\n",
    "        titles.append(title)\n",
    "        prices.append(price)\n",
    "        urls.append(url)\n",
    "        count += 1   \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Price': prices,\n",
    "        'Image Url': urls,\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52fbf8-0c54-4eed-a850-685696854505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please visit https://www.cnbc.com/world/?region=world and scrap a) headings b) date c) News link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9854bf47-39b2-446d-b29e-8350317bd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Heading         Date  \\\n",
      "0   Shooting attack at the West Bank-Jordan border...         None   \n",
      "1   CIA director saw ‘genuine risk’ of Russia usin...         None   \n",
      "2   Boeing and union reach a labor deal, potential...  2 hours ago   \n",
      "3   Venezuela opposition candidate Gonzalez leaves...         None   \n",
      "4   The bitcoin ATM has emerged as one of crypto's...  2 hours ago   \n",
      "5   I reached financial freedom at the age of 38: ...         None   \n",
      "6   Bull market limps into 2-year birthday with so...         None   \n",
      "7   Small nuclear reactors could power the future ...         None   \n",
      "8   Dell and Palantir to join S&P 500; shares of b...         None   \n",
      "9   White House requests investigation of American...         None   \n",
      "10  Coinbase has worst week of the year as crypto ...         None   \n",
      "11  AI craze is distorting VC market, as tech gian...         None   \n",
      "12  Trump is no threat to U.S. democracy, accordin...         None   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2024/09/08/shooting-attac...  \n",
      "1   https://www.cnbc.com/2024/09/07/cia-director-r...  \n",
      "2   https://www.cnbc.com/2024/09/08/boeing-and-uni...  \n",
      "3   https://www.cnbc.com/2024/09/08/venezuela-oppo...  \n",
      "4   https://www.cnbc.com/2024/09/08/biggest-risks-...  \n",
      "5   https://www.cnbc.com/2024/09/04/here-are-the-f...  \n",
      "6   https://www.cnbc.com/2024/09/07/bull-market-li...  \n",
      "7   https://www.cnbc.com/2024/09/07/how-small-modu...  \n",
      "8   https://www.cnbc.com/2024/09/06/dell-and-palan...  \n",
      "9   https://www.cnbc.com/2024/09/07/white-house-re...  \n",
      "10  https://www.cnbc.com/2024/09/06/coinbase-marat...  \n",
      "11  https://www.cnbc.com/2024/09/06/ai-craze-getti...  \n",
      "12  https://www.cnbc.com/2024/09/06/trump-is-no-th...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    headings = []\n",
    "    dates = []\n",
    "    links = []\n",
    "\n",
    "    news = soup.find('div', class_='RiverPlus-riverPlusContainer').find_all('div', class_='RiverPlusCard-container')\n",
    "\n",
    "    for item in news:\n",
    "\n",
    "        heading_tag = item.find('div', class_='RiverHeadline-headline').find('a')\n",
    "        heading = heading_tag.get_text(strip=True)\n",
    "        link = heading_tag['href'] \n",
    "\n",
    "        date_tag = item.find('span', class_='RiverByline-datePublished')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    " \n",
    "        if heading:\n",
    "           headings.append(heading) \n",
    "           dates.append(date)\n",
    "           links.append(link) \n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Heading': headings,\n",
    "        'Date': dates,\n",
    "        'Link': links,\n",
    "    })\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2fafa-5d9b-421b-91f4-e6ac3cfe9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/and scrap. a) Paper title b) date c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4460800f-3141-4504-aa54-b7a019898629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title            Date  \\\n",
      "0   Implementation of artificial intelligence in a...            2020   \n",
      "1               Review of agricultural IoT technology            2022   \n",
      "2   Automation and digitization of agriculture usi...            2021   \n",
      "3   A comprehensive review on automation in agricu...       June 2019   \n",
      "4   Applications of electronic nose (e-nose) and e...            2020   \n",
      "5   Towards sustainable agriculture: Harnessing AI...       June 2024   \n",
      "6             Fruit ripeness classification: A survey      March 2023   \n",
      "7   Deep learning based computer vision approaches...            2022   \n",
      "8   A review of imaging techniques for plant disea...            2020   \n",
      "9   Transfer Learning for Multi-Crop Leaf Disease ...            2022   \n",
      "10  Comparison of CNN-based deep learning architec...  September 2023   \n",
      "11  DeepRice: A deep learning and deep feature bas...      March 2024   \n",
      "12  Computer vision in smart agriculture and preci...  September 2024   \n",
      "13  Image classification on smart agriculture plat...  September 2024   \n",
      "14  Using an improved lightweight YOLOv8 model for...      March 2024   \n",
      "15  How artificial intelligence uses to achieve th...       June 2023   \n",
      "16  LeafSpotNet: A deep learning framework for det...       June 2024   \n",
      "17  Plant disease detection using hybrid model bas...            2021   \n",
      "18  Cross-comparative review of Machine learning f...       June 2024   \n",
      "19  A comprehensive survey on weed and crop classi...  September 2024   \n",
      "20  A systematic review of machine learning techni...            2022   \n",
      "21  Machine learning in nutrient management: A review  September 2023   \n",
      "22  Deep convolutional neural network models for w...            2022   \n",
      "23  A review on computer vision systems in monitor...            2020   \n",
      "24  Explainable artificial intelligence and interp...            2022   \n",
      "\n",
      "                                              Authors  \n",
      "0   Tanha Talaviya |  Dhara Shah |  Nivedita Patel...  \n",
      "1          Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian  \n",
      "2                            A. Subeesh |  C.R. Mehta  \n",
      "3   Kirtan Jha |  Aalap Doshi |  Poojan Patel |  M...  \n",
      "4                               Juzhong Tan |  Jie Xu  \n",
      "5                 Dhananjay K. Pandey |  Richa Mishra  \n",
      "6   Matteo Rizzo |  Matteo Marcuzzo |  Alessandro ...  \n",
      "7   V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha |  ...  \n",
      "8        Vijai Singh |  Namita Sharma |  Shikha Singh  \n",
      "9              Ananda S. Paymode |  Vandana B. Malode  \n",
      "10  Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid ...  \n",
      "11  P. Isaac Ritharson |  Kumudha Raimond |  X. An...  \n",
      "12  Sumaira Ghazal |  Arslan Munir |  Waqar S. Qur...  \n",
      "13  Juan Felipe Restrepo-Arias |  John W. Branch-B...  \n",
      "14  Baoling Ma |  Zhixin Hua |  Yuchen Wen |  Hong...  \n",
      "15            Vilani Sachithra |  L.D.C.S. Subhashini  \n",
      "16         Shwetha V |  Arnav Bhagwat |  Vijaya Laxmi  \n",
      "17                         Punam Bedi |  Pushkar Gole  \n",
      "18  James Daniel Omaye |  Emeka Ogbuju |  Grace At...  \n",
      "19  Faisal Dharma Adhinata |   Wahyono |  Raden Su...  \n",
      "20  Md Ekramul Hossain |  Muhammad Ashad Kabir |  ...  \n",
      "21  Oumnia Ennaji |  Leonardus Vergütz |  Achraf E...  \n",
      "22  A. Subeesh |  S. Bhole |  K. Singh |  N.S. Cha...  \n",
      "23  Cedric Okinda |  Innocent Nyalala |  Tchalla K...  \n",
      "24                                       Masahiro Ryo  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "\n",
    "    articles = soup.find_all('div', class_='article-listing')\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        title_tag = article.find('h2', class_='h5 article-title').find('a')\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "        date_tag = article.find('p', class_='article-date')\n",
    "        date = date_tag.get_text(strip=True)\n",
    "\n",
    "        author_tag = article.find('p', class_='article-authors')\n",
    "        author = author_tag.get_text(strip=True)\n",
    "\n",
    "        # Append data to lists\n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        authors.append(author) \n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Date': dates,\n",
    "        'Authors': authors,\n",
    "    })\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de70d1-8b6d-4415-8c65-69c81d9ac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 : In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d5eb7d44-0cde-4917-b6ec-2cb8215452c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Job Title  \\\n",
      "0                                  Data Scientist   \n",
      "1  Data Scientist (AI/ML)|| US Based MNC || Noida   \n",
      "2                                  Data Scientist   \n",
      "3                                  Data Scientist   \n",
      "4                                  Data scientist   \n",
      "5               Data Scientist/Data Analyst - LLM   \n",
      "6                                  Data Scientist   \n",
      "7                                  Data Scientist   \n",
      "8                                  Data Scientist   \n",
      "9                          Data Scientist (Telco)   \n",
      "\n",
      "                                            Location  \\\n",
      "0  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "1                                     Hybrid - Noida   \n",
      "2                                           Gurugram   \n",
      "3                                          New Delhi   \n",
      "4                                           Gurugram   \n",
      "5  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "6                                           Gurugram   \n",
      "7                                          Ghaziabad   \n",
      "8                                              Noida   \n",
      "9                                Gurugram, Bengaluru   \n",
      "\n",
      "                Company Name Experience Required  \n",
      "0             Nityo Infotech             3-7 Yrs  \n",
      "1  US based Software Company             3-5 Yrs  \n",
      "2               Collegedunia             0-2 Yrs  \n",
      "3                   Sociomix             0-5 Yrs  \n",
      "4               Growthjockey             0-1 Yrs  \n",
      "5               Hexaconcepts             2-6 Yrs  \n",
      "6       Biz Tech Consultants             3-8 Yrs  \n",
      "7       Biz Tech Consultants             3-8 Yrs  \n",
      "8       Biz Tech Consultants             3-8 Yrs  \n",
      "9                       PayU             2-7 Yrs  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the Naukri page\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Step 2: Enter ‚ÄúData Scientist‚Äù in the search field\n",
    "search_box = driver.find_element(By.XPATH, \"//input[@placeholder='Enter skills / designations / companies']\")\n",
    "search_box.send_keys(\"Data Scientist\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Apply location filter for Delhi/NCR\n",
    "location_filter = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[text()='Delhi / NCR']\"))\n",
    ")\n",
    "location_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Apply salary filter for 3-6 Lakhs\n",
    "salary_filter = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[text()='3-6 Lakhs']\"))\n",
    ")\n",
    "salary_filter.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5: Scrape data for the first 10 jobs\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find job listings\n",
    "jobs = soup.find_all('div', class_='srp-jobtuple-wrapper')[:10]\n",
    "\n",
    "# Data lists\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Loop through the job results and extract the required details\n",
    "for job in jobs:\n",
    "    # Job title\n",
    "    title = job.find('a', class_='title').text.strip()\n",
    "    job_titles.append(title)\n",
    "\n",
    "    # Job location\n",
    "    location = job.find('span', class_='locWdth').text.strip()\n",
    "    job_locations.append(location)\n",
    "\n",
    "    # Company name\n",
    "    company = job.find('a', class_='comp-name').text.strip()\n",
    "    company_names.append(company)\n",
    "\n",
    "    # Experience required\n",
    "    experience = job.find('span', class_='expwdth').text.strip()\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04550de-a787-4e09-a1c0-fa5e7fd69ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 : Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52b86224-9a0b-477f-9445-0308c97229fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Data Analyst , Senior Data Analyst , Data Anal...   \n",
      "1                                       Data Analyst   \n",
      "2    Risk Data Analyst (Senior Quantitative Analyst)   \n",
      "3                            Healthcare Data Analyst   \n",
      "4                                Senior Data Analyst   \n",
      "5                     Urgent Hiring For Data Analyst   \n",
      "6                   Data Catalog with Data Goverance   \n",
      "7                            Data Governance Analyst   \n",
      "8                                 Healthcare Analyst   \n",
      "9                             Data Science Analytics   \n",
      "\n",
      "                                            Location  \\\n",
      "0  Bangalore+8Chennai, Noida, Hyderabad, Gurugram...   \n",
      "1       Bangalore+4Kolkata, Pune, Mumbai City, Delhi   \n",
      "2                                 Bangalore+1Kolkata   \n",
      "3  Bangalore+8Noida, Chennai, Hyderabad, Kolkata,...   \n",
      "4  Bangalore+8Noida, Chennai, Hyderabad, Kolkata,...   \n",
      "5                     Bangalore+2Chennai, Coimbatore   \n",
      "6  Bangalore+8Noida, Chennai, Hyderabad, Gurugram...   \n",
      "7                                          Bangalore   \n",
      "8  Bangalore+7Chennai, United Arab Emirates, Hyde...   \n",
      "9  Bangalore+7Noida, Chennai, Hyderabad, Gurugram...   \n",
      "\n",
      "                             Company Name Experience Required  \n",
      "0                       AppSoft Solutions          0 to 4 Yrs  \n",
      "1                        ARYAN TECHNOLOGY          0 to 4 Yrs  \n",
      "2                       locus enterprises          4 to 9 Yrs  \n",
      "3               SPENTO PAPERS (INDIA) LLP         8 to 13 Yrs  \n",
      "4     GALLAGHER AND MOHAN PRIVATE LIMITED          2 to 7 Yrs  \n",
      "5  DIRAA HR SERVICES Hiring For DIRAA ...          0 to 4 Yrs  \n",
      "6                     LTIMindtree Limited         6 to 11 Yrs  \n",
      "7  WHITE HORSE MANPOWER CONSULTANCY (P...          5 to 8 Yrs  \n",
      "8                           TECHNO ENDURA          0 to 4 Yrs  \n",
      "9  MACKENZIE MODERN IT SOLUTIONS PRIVA...          5 to 8 Yrs  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the Shine website\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Step 2: Enter \"Data Analyst\" in the job title field\n",
    "search_field = driver.find_element(By.XPATH, \"//input[@placeholder='Job title, skills']\")\n",
    "search_field.click()\n",
    "time.sleep(2)\n",
    "job_title_field = driver.find_element(By.ID, \"id_q\") \n",
    "job_title_field.send_keys(\"Data Analyst\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Enter \"Bangalore\" in the location field\n",
    "location_field = driver.find_element(By.ID, \"id_loc\") \n",
    "location_field.send_keys(\"Bangalore\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 4: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[text()='Search jobs']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5: Scrape the data for the first 10 job results\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find job listings\n",
    "# jobs = soup.find('div', class_='parentClass').find_all('div', class_='')[:10]\n",
    "jobs = soup.find_all('div', class_='jobCard')[:10]\n",
    "\n",
    "# Lists to store job data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Loop through the first 10 jobs and extract required details\n",
    "for job in jobs:\n",
    "    # Job title\n",
    "    title = job.find('strong', class_='jobCard_pReplaceH2__xWmHg').find('a', class_=False).text.strip()\n",
    "    job_titles.append(title)\n",
    "\n",
    "    # Job location\n",
    "    location = job.find('div', class_='jobCard_locationIcon__zrWt2').text.strip()\n",
    "    job_locations.append(location)\n",
    "\n",
    "    # Company name\n",
    "    company = job.find('div', class_='jobCard_jobCard_cName__mYnow').find('span').text.strip()\n",
    "    company_names.append(company)\n",
    "\n",
    "    # Experience required\n",
    "    experience = job.find('div', class_='jobCard_jobIcon__3FB1t').text.strip()\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6735f39-bac7-4616-b98b-51ae390c0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2a74d379-18e0-4e9a-a853-d8350c0cad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rating              Reviews  \\\n",
      "0      5  Best in the market!   \n",
      "1      5             Terrific   \n",
      "2      5       Classy product   \n",
      "3      5    Worth every penny   \n",
      "4      5     Perfect product!   \n",
      "5      5    Terrific purchase   \n",
      "6      5            Wonderful   \n",
      "7      5            Excellent   \n",
      "8      5            Brilliant   \n",
      "9      5     Perfect product!   \n",
      "\n",
      "                                        Full Reviews  \n",
      "0                               Good CameraREAD MORE  \n",
      "1                            Very very goodREAD MORE  \n",
      "2  Camera is awesomeBest battery backupA performe...  \n",
      "3  Feeling awesome after getting the delivery of ...  \n",
      "4                              Photos superREAD MORE  \n",
      "5                         Value for money üòçREAD MORE  \n",
      "6                    This is amazing at allREAD MORE  \n",
      "7                                       NYCREAD MORE  \n",
      "8                  very good camera qualityREAD MORE  \n",
      "9                                V Good allREAD MORE  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Get the website\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "data = soup.find_all('div', class_='cPHDOP')\n",
    "\n",
    "ratings = []\n",
    "reviews = []\n",
    "full_reviews = []\n",
    "\n",
    "for item in data[1:]:\n",
    "    rating_element = item.find('div', class_='XQDdHH')\n",
    "    \n",
    "    if rating_element:\n",
    "        rating = rating_element.text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "    review_element = item.find('p', class_='z9E0IG')\n",
    "    \n",
    "    if review_element:\n",
    "        review = review_element.text.strip()\n",
    "        reviews.append(review)\n",
    "\n",
    "\n",
    "    full_review_element = item.find('div', class_='ZmyHeo')\n",
    "    \n",
    "    if review_element:\n",
    "        full_review = full_review_element.text.strip()\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "# Create a DataFrame\n",
    "dataobject = {\n",
    "    'Rating': ratings,\n",
    "    'Reviews': reviews,\n",
    "    'Full Reviews': full_reviews\n",
    "}\n",
    "df = pd.DataFrame(dataobject)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5f148-fe88-47cd-86bf-96b7a1627e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search field. You have to scrape 3 attributes of each sneaker: 1. Brand 2. Product Description 3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d2e7b05-2f7c-4c64-8ac1-30765e8d6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login popup did not appear.\n",
      "             Brand                                Product Description    Price\n",
      "0             Bata                         DASH E 24 Sneakers For Men     ‚Çπ549\n",
      "1            Sparx   SM 852 | Stylish, Comfortable | Sneakers For Men     ‚Çπ879\n",
      "2   TOMMY HILFIGER                                   Sneakers For Men   ‚Çπ3,609\n",
      "3           CAMPUS                         BROWNIE Sneakers For Women   ‚Çπ1,374\n",
      "4         Vellinto            Casual Sneakrs For Men Sneakers For Men     ‚Çπ599\n",
      "5             Bata                         DUNK E 24 Sneakers For Men     ‚Çπ604\n",
      "6            WROGN                                   Sneakers For Men     ‚Çπ909\n",
      "7            WROGN                                   Sneakers For Men   ‚Çπ1,049\n",
      "8           KILLER                        KLR-009 01 Sneakers For Men   ‚Çπ1,680\n",
      "9            Abros                             ORBIT Sneakers For Men     ‚Çπ524\n",
      "10       Deals4you                                 Sneakers For Women     ‚Çπ439\n",
      "11           Zixer  Exclusive dancing shoe for boyÔøΩs, Funky dancin...     ‚Çπ549\n",
      "12           WROGN                                   Sneakers For Men     ‚Çπ734\n",
      "13  FLYING MACHINE                         CARPE 2.0 Sneakers For Men   ‚Çπ1,199\n",
      "14        Roadster                            Casual Sneakers For Men   ‚Çπ1,403\n",
      "15        Red Tape  Casual Sneaker Shoes For Women | Stylish and C...   ‚Çπ1,079\n",
      "16        BERSACHE  Bersache Sneaker, Loafers ,Casual With Extra C...     ‚Çπ658\n",
      "17          Footox                    Casual Shoes Sneakers For Women     ‚Çπ449\n",
      "18           asian  Blossom-06 Gym,Sports,Training,Stylish with Ex...     ‚Çπ791\n",
      "19        Roadster                                   Sneakers For Men   ‚Çπ1,517\n",
      "20            Bata                                   Sneakers For Men     ‚Çπ439\n",
      "21            Bata                                   Sneakers For Men     ‚Çπ439\n",
      "22        Skechers                        GLIDE-STEP Sneakers For Men   ‚Çπ3,369\n",
      "23           WROGN                                   Sneakers For Men     ‚Çπ804\n",
      "24  TOMMY HILFIGER                                   Sneakers For Men   ‚Çπ4,583\n",
      "25    UNDER ARMOUR                                   Sneakers For Men  ‚Çπ13,799\n",
      "26            Bata                      Viho Slip On Sneakers For Men     ‚Çπ764\n",
      "27           Zixer  Exclusive Rap & Dance Shoes For Boys Sneakers ...     ‚Çπ649\n",
      "28           asian  Casual Sneaker Shoes for Men | Soft Cushioned ...   ‚Çπ1,272\n",
      "29        Red Tape  Casual Sneaker Shoes for Men | Elegantly Round...   ‚Çπ1,709\n",
      "30           Arivo  Black Outdoor Gym,Sports,Training,Stylish Runn...     ‚Çπ699\n",
      "31        BERSACHE  Sneaker, Loafers ,Casual With Extra Comfort Sn...     ‚Çπ661\n",
      "32           asian  Flat Sole Sneakers Shoes for all Day wear with...   ‚Çπ1,461\n",
      "33        WOODLAND                                   Sneakers For Men   ‚Çπ1,428\n",
      "34            FILA                                   Sneakers For Men     ‚Çπ800\n",
      "35          HASTEN                                   Sneakers For Men     ‚Çπ699\n",
      "36        BERSACHE  Bersache Sneaker, Loafers ,Casual With Extra C...     ‚Çπ628\n",
      "37            Bata                         ADAM E 24 Sneakers For Men     ‚Çπ549\n",
      "38           WROGN                                   Sneakers For Men   ‚Çπ1,049\n",
      "39            PUMA                    Trackracer 3.0 Sneakers For Men   ‚Çπ2,115\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the website\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "try:\n",
    "    close_button = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), '‚úï')]\"))\n",
    "    )\n",
    "    close_button.click()\n",
    "except Exception as e:\n",
    "    print(\"Login popup did not appear.\")\n",
    "\n",
    "# Step 2: Enter \"sneakers\" in the search field\n",
    "search_box = driver.find_element(By.NAME, \"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize lists to store data\n",
    "brands = []\n",
    "product_descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Get the page source and parse it with BeautifulSoup\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find sneaker elements\n",
    "sneaker_elements = soup.find_all('div', class_='cPHDOP')\n",
    "\n",
    "for sneaker in sneaker_elements:\n",
    "    inner = sneaker.find_all('div', class_='LFEi7Z')\n",
    "    \n",
    "    for item in inner:\n",
    "        # Extract brand\n",
    "        brand = item.find('div', class_='syl9yP')\n",
    "        if brand:\n",
    "            brands.append(brand.text.strip())\n",
    "    \n",
    "        # Extract product description\n",
    "        description = item.find('a', class_='WKTcLC')\n",
    "        if description:\n",
    "            product_descriptions.append(description.text.strip())\n",
    "    \n",
    "        # Extract price\n",
    "        price = item.find('div', class_='Nx9bqj')\n",
    "        if price:\n",
    "            prices.append(price.text.strip())\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "data = {\n",
    "    'Brand': brands,\n",
    "    'Product Description': product_descriptions,\n",
    "    'Price': prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fe9ce-1639-4f4d-bae9-376f9aceac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5:Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a8436ea-ae00-4c0e-af5a-cf9f4487b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title    Ratings     Price\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...               59,990\n",
      "1  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...               76,990\n",
      "2  Dell [Smartchoice] Inspiron 5430 Thin & Light ...               75,490\n",
      "3  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...               49,990\n",
      "4  ASUS Vivobook 15, 15.6\" (39.62cm) FHD, Intel C...               61,990\n",
      "5  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...  No Rating    71,990\n",
      "6  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...               55,990\n",
      "7  ASUS TUF Gaming F15, 15.6\" (39.62cm) FHD 144Hz...             1,08,048\n",
      "8  (Refurbished) Dell Latitude 7480 14in FHD Lapt...               27,531\n",
      "9  Acer Aspire 3 Intel Core i7 12th Gen 1255U - (...  No Rating    57,990\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the Amazon website\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Step 2: Enter \"Laptop\" in the search field\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "\n",
    "# Step 3: Click the search icon\n",
    "search_button = driver.find_element(By.ID, \"nav-search-submit-button\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Set the CPU Type filter to \"Intel Core i7\"\n",
    "cpu_filter = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[text()='Intel Core i7']\"))\n",
    ")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Wait for the results page to load with the filter applied\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize lists to store data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Step 5: Scrape the data for the first 10 laptops\n",
    "laptop_elements = driver.find_elements(By.XPATH, \"//div[contains(@data-component-type, 's-search-result')]\")[:10]\n",
    "\n",
    "for laptop in laptop_elements:\n",
    "    # Extract title\n",
    "    title_element = laptop.find_element(By.XPATH, \".//h2/a/span\")\n",
    "    title = title_element.text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Extract ratings\n",
    "    try:\n",
    "        rating_element = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\")\n",
    "        rating = rating_element.text.strip()\n",
    "    except:\n",
    "        rating = \"No Rating\"  \n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Extract price\n",
    "    try:\n",
    "        price_element = laptop.find_element(By.XPATH, \".//span[@class='a-price-whole']\")\n",
    "        price = price_element.text.strip()\n",
    "    except:\n",
    "        price = \"Price Not Available\"  \n",
    "    prices.append(price)\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cecc74-6081-47b3-bc66-ceb8d5c651f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Write a python program to scrape data for Top 1000 Quotes of All Time. The above task will be done in following steps: 1. First get the webpagehttps://www.azquotes.com/ 2. Click on Top Quote 3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9307d8d1-e8cf-474d-81b8-5a3fb20c19ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to scrape.\n",
      "                                                 Quote              Author  \\\n",
      "0    The essence of strategy is choosing what not t...      Michael Porter   \n",
      "1    One cannot and must not try to erase the past ...          Golda Meir   \n",
      "2    Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
      "3    Death is something inevitable. When a man has ...      Nelson Mandela   \n",
      "4    You have to love a nation that celebrates its ...        Erma Bombeck   \n",
      "..                                                 ...                 ...   \n",
      "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
      "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
      "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
      "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
      "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
      "\n",
      "                                Type of Quote  \n",
      "0    Essence, Deep Thought, Transcendentalism  \n",
      "1                   Inspiration, Past, Trying  \n",
      "2                         Country, Peace, War  \n",
      "3          Inspirational, Motivational, Death  \n",
      "4                4th Of July, Food, Patriotic  \n",
      "..                                        ...  \n",
      "995         Love, Inspirational, Motivational  \n",
      "996                    Gun, Two, Qualms About  \n",
      "997     Inspirational, Greatness, Best Effort  \n",
      "998                    Spiritual, Truth, Yoga  \n",
      "999      Inspirational, Leadership, Education  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the AzQuotes webpage\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "# Step 2: Click on Top Quotes\n",
    "top_quotes_link = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'Top Quotes')]\"))\n",
    ")\n",
    "top_quotes_link.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize lists to store data\n",
    "quotes = []\n",
    "authors = []\n",
    "types_of_quotes = []\n",
    "\n",
    "# Step 3: Scrape quotes data\n",
    "# Loop to scrape top 1000 quotes (approximately)\n",
    "for _ in range(10):  # 10 pages, assuming 100 quotes per page\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all quote elements\n",
    "    quote_elements = soup.find('ul', class_='list-quotes').find_all('li', class_=False)\n",
    "\n",
    "    for quote_element in quote_elements:\n",
    "        # Extract quote\n",
    "        quote = quote_element.find('a', class_='title').text.strip()\n",
    "        quotes.append(quote)\n",
    "\n",
    "        # Extract author\n",
    "        author = quote_element.find('div', class_='author').find('a').text.strip()\n",
    "        authors.append(author)\n",
    "\n",
    "        # Extract type of quote\n",
    "        type_of_quote = quote_element.find('div', class_='tags').text.strip() if quote_element.find('div', class_='tags') else 'N/A'\n",
    "        types_of_quotes.append(type_of_quote)\n",
    "\n",
    "    # Click on the next page if available\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[@class='next']/a\")\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Wait for the next page to load\n",
    "    except Exception as e:\n",
    "        print(\"No more pages to scrape.\")\n",
    "        break\n",
    "\n",
    "# Step 4: Create a DataFrame\n",
    "data = {\n",
    "    'Quote': quotes,\n",
    "    'Author': authors,\n",
    "    'Type of Quote': types_of_quotes\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda1e54-9cec-4382-bb23-28b2d66cd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcf6693-7a22-450f-92e7-8f01fda50a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name           Born-Dead  \\\n",
      "0          Jawaharlal Nehru  57 years, 274 days   \n",
      "1          Gulzarilal Nanda  65 years, 328 days   \n",
      "2       Lal Bahadur Shastri  62 years, 250 days   \n",
      "3          Gulzarilal Nanda  67 years, 191 days   \n",
      "4             Indira Gandhi   48 years, 66 days   \n",
      "5             Morarji Desai   81 years, 24 days   \n",
      "6              Charan Singh  76 years, 217 days   \n",
      "7             Indira Gandhi   62 years, 56 days   \n",
      "8              Rajiv Gandhi   40 years, 72 days   \n",
      "9   Vishwanath Pratap Singh  58 years, 160 days   \n",
      "10          Chandra Shekhar  63 years, 207 days   \n",
      "11      P. V. Narasimha Rao  69 years, 358 days   \n",
      "12     Atal Bihari Vajpayee  71 years, 143 days   \n",
      "13         H. D. Deve Gowda   63 years, 14 days   \n",
      "14       Inder Kumar Gujral  77 years, 138 days   \n",
      "15     Atal Bihari Vajpayee   73 years, 84 days   \n",
      "16           Manmohan Singh  71 years, 239 days   \n",
      "17            Narendra Modi  63 years, 251 days   \n",
      "\n",
      "                         Term of Office             Remarks  \n",
      "0          (15 August1947, 27 May 1964)  16 years, 286 days  \n",
      "1            (27 May 1964, 9 June 1964)             13 days  \n",
      "2        (9 June 1964, 11 January 1966)    1 year, 216 days  \n",
      "3    (11 January 1966, 24 January 1966)             13 days  \n",
      "4      (24 January 1966, 24 March 1977)   11 years, 59 days  \n",
      "5         (24 March 1977, 28 July 1979)   2 years, 126 days  \n",
      "6       (28 July 1979, 14 January 1980)            170 days  \n",
      "7    (14 January 1980, 31 October 1984)   4 years, 291 days  \n",
      "8    (31 October 1984, 2 December 1989)    5 years, 32 days  \n",
      "9   (2 December 1989, 10 November 1990)            343 days  \n",
      "10     (10 November 1990, 21 June 1991)            223 days  \n",
      "11          (21 June 1991, 16 May 1996)   4 years, 330 days  \n",
      "12           (16 May 1996, 1 June 1996)             16 days  \n",
      "13         (1 June 1996, 21 April 1997)            324 days  \n",
      "14       (21 April 1997, 19 March 1998)            332 days  \n",
      "15         (19 March 1998, 22 May 2004)    6 years, 64 days  \n",
      "16           (22 May 2004, 26 May 2014)    10 years, 4 days  \n",
      "17             (26 May 2014, Incumbent)   10 years, 87 days  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# Disable images\n",
    "prefs = {\n",
    "    \"profile.managed_default_content_settings.images\": 2,  # Disable loading images\n",
    "    \"profile.managed_default_content_settings.stylesheets\": 2,  # Disable CSS\n",
    "    \"profile.managed_default_content_settings.javascript\": 1,  # Keep JS enabled to load essential content\n",
    "}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Initialize WebDriver with the configured options\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Allow some time for the page to load\n",
    "time.sleep(3)  \n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the table containing Prime Ministers\n",
    "table = soup.find('table', class_=False)\n",
    "\n",
    "# Initialize lists to hold data\n",
    "names = []\n",
    "born_dead = []\n",
    "terms = []\n",
    "remarks = []\n",
    "\n",
    "# Iterate through the rows of the table\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]: \n",
    "    cols = row.find_all('td')\n",
    "\n",
    "    if len(cols) == 6:\n",
    "        names.append(cols[1].text.strip())\n",
    "        born_dead.append(cols[2].text.strip())\n",
    "        terms.append((cols[3].text.strip(), cols[4].text.strip()))\n",
    "        remarks.append(cols[5].text.strip())\n",
    "# Create a DataFrame\n",
    "pm_data = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms,\n",
    "    'Remarks': remarks\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(pm_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69cfc4-49c9-4d8b-a47c-027d2a6592e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd35b9e7-ae72-4f9d-a66f-33d41e755afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Car Name  \\\n",
      "0                           McLaren Senna GTR   \n",
      "1                                 Czinger 21C   \n",
      "2                               Ferrari Monza   \n",
      "3                          Gordon Murray T.33   \n",
      "4                           Koenigsegg Gemera   \n",
      "5                          Hennessey Venom F5   \n",
      "6                             Bentley Bacalar   \n",
      "7               Hispano Suiza Carmen Boulogne   \n",
      "8                      Bentley Mulliner Batur   \n",
      "9                                 SSC Tuatara   \n",
      "10                                Lotus Evija   \n",
      "11                        Aston Martin Vulcan   \n",
      "12                                 Delage D12   \n",
      "13                        Ferrari Daytona SP3   \n",
      "14                          McLaren Speedtail   \n",
      "15                               Rimac Nevera   \n",
      "16                              Pagani Utopia   \n",
      "17                       Pininfarina Battista   \n",
      "18                         Gordon Murray T.50   \n",
      "19                       Lamborghini Countach   \n",
      "20              Hennessey Venom F5 Revolution   \n",
      "21                   Mercedes-AMG Project One   \n",
      "22                               Zenvo Aurora   \n",
      "23                        Aston Martin Victor   \n",
      "24                Hennessey Venom F5 Roadster   \n",
      "25                           Koenigsegg Jesko   \n",
      "26                                 Aspark Owl   \n",
      "27                      Aston Martin Valkyrie   \n",
      "28                  W Motors Lykan Hypersport   \n",
      "29                              McLaren Solus   \n",
      "30                        Pagani Huayra Evo R   \n",
      "31                           Lamborghini Sian   \n",
      "32                           Koenigsegg CC850   \n",
      "33            Bugatti Chiron Super Sport 300+   \n",
      "34  Gordon Murray Automotive T.50s Niki Lauda   \n",
      "35                  Pagani Huayra Roadster BC   \n",
      "36                         Lamborghini Veneno   \n",
      "37                             Bugatti Bolide   \n",
      "38                  Pininfarina B95 Speedster   \n",
      "39                            Bugatti Mistral   \n",
      "40                               Bugatti Divo   \n",
      "41                        Pagani Huayra Imola   \n",
      "42                           Pagani Codalunga   \n",
      "43                   Mercedes-Maybach Exelero   \n",
      "44                         Bugatti Centodieci   \n",
      "45                    Bugatti Chiron Profil√©e   \n",
      "46                       Rolls-Royce Sweptail   \n",
      "47                   Bugatti La Voiture Noire   \n",
      "48                      Rolls-Royce Boat Tail   \n",
      "49         Rolls-Royce La Rose Noire Droptail   \n",
      "\n",
      "                                                Price  \n",
      "0   When you think of pricey supercars, a few comp...  \n",
      "1   But to find out which of these unattainable-to...  \n",
      "2   It should be noted, though, that the prices li...  \n",
      "3                                        $1.7 Million  \n",
      "4   The McLaren Senna GTR costs nearly $2 million,...  \n",
      "5                                 Price: $1.7 Million  \n",
      "6   You might not know the name Czinger yet, but t...  \n",
      "7                                 Price: $1.7 Million  \n",
      "8   Much like the roof-less McLaren Elva, the Ferr...  \n",
      "9                                 Price: $1.7 Million  \n",
      "10  The second and slightly more affordable superc...  \n",
      "11                                Price: $1.7 Million  \n",
      "12  One of two Koenigsegg models on this list, the...  \n",
      "13                                Price: $1.8 Million  \n",
      "14  The Hennessey Venom GT was a record-breaker, t...  \n",
      "15                                Price: $1.9 Million  \n",
      "16  With just 12 total units produced, the Bentley...  \n",
      "17                                Price: $1.9 Million  \n",
      "18  To call the Hispano Suiza Carmen Boulogne beau...  \n",
      "19                                Price: $2.0 Million  \n",
      "20  The electric onslaught is coming. Bentley says...  \n",
      "21                                Price: $2.0 Million  \n",
      "22  Although initially cloaked in controversy, SSC...  \n",
      "23                                Price: $2.1 Million  \n",
      "24  With a new Emira sports car and an Eletre elec...  \n",
      "25                                Price: $2.3 Million  \n",
      "26  As with a few other cars on this list, the Ast...  \n",
      "27                                Price: $2.3 Million  \n",
      "28  You may have heard of Delage before. In the ea...  \n",
      "29                                Price: $2.3 Million  \n",
      "30  The Ferrari Daytona SP3 is the brand‚Äôs most ae...  \n",
      "31                                Price: $2.3 Million  \n",
      "32  What would you pay for the fastest production ...  \n",
      "33                                Price: $2.4 Million  \n",
      "34  The Rimac Nevera takes the title of most expen...  \n",
      "35                                Price: $2.5 Million  \n",
      "36  First came the Zonda, then the Huayra, and now...  \n",
      "37                                Price: $2.5 Million  \n",
      "38  Aptly named after the company‚Äôs founder, Batti...  \n",
      "39                                Price: $2.6 Million  \n",
      "40  If the name Gordon Murray sounds familiar, it‚Äô...  \n",
      "41                                Price: $2.6 Million  \n",
      "42  The name Countach may be iconic, but is it wor...  \n",
      "43                                       $2.7 Million  \n",
      "44  With the new F5, Hennessey moved into building...  \n",
      "45                                Price: $2.7 Million  \n",
      "46  Mercedes has promised a production version of ...  \n",
      "47                                Price: $2.8 Million  \n",
      "48  Zenvo wowed at Monterey Car Week 2023 with the...  \n",
      "49                                Price: $3.0 Million  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(\"C:/Users/mahes/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\") \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 2: Find the search bar and type '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, 'q')  \n",
    "search_bar.send_keys(\"50 most expensive cars\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 3: Click on the first relevant result related to \"50 most expensive cars\"\n",
    "article_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"50 Most Expensive Cars\")\n",
    "article_link.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the car names and prices from the article\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "cars = driver.find_elements(By.CSS_SELECTOR, 'h3') \n",
    "prices = driver.find_elements(By.CSS_SELECTOR, 'p') \n",
    "\n",
    "# Extract car names and prices\n",
    "for car in cars[:50]: \n",
    "    car_names.append(car.text)\n",
    "\n",
    "for price in prices[:50]: \n",
    "    car_prices.append(price.text)\n",
    "\n",
    "# Step 5: Create a Pandas DataFrame\n",
    "data = {\n",
    "    \"Car Name\": car_names,\n",
    "    \"Price\": car_prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
